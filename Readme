RAG with Elastic + Open LLM ðŸš€

A simplified Retrieval-Augmented Generation (RAG) system built for the internship project.
It uses Elasticsearch (BM25, dense embeddings, and ELSER sparse retrieval) plus an open-source LLM (via Ollama/HuggingFace) to answer questions over PDF documents stored in Google Drive.
The project provides both a FastAPI backend and a Streamlit UI, returning grounded answers with citations (filename, link, snippet, page, heading, etc.).

âœ¨ Features

Ingestion

Load PDFs from local folder or Google Drive (file/folder link).

Split text into ~300-token chunks with overlap.

Extract metadata: filename, url, chunk_id, page, heading, section, part_section.

Index into Elasticsearch with:

ELSER sparse embeddings â†’ ml.tokens.

Dense embeddings â†’ 384-dim MiniLM.

Raw text â†’ BM25 keyword search.

Retrieval

ELSER-only mode.

Hybrid mode â†’ Reciprocal Rank Fusion (ELSER + dense + BM25).

Configurable Top K (default = 5).

Answer Generation

Local/Open LLM via Ollama (default: llama3).

Grounded responses only (refuses to hallucinate).

Guardrails: rejects unsafe/harmful queries.

API (FastAPI)

POST /query â†’ question â†’ returns answer + citations.

POST /query_debug â†’ returns raw retrieved chunks with scores.

POST /ingest â†’ ingest PDFs from local folder.

POST /ingest_drive â†’ ingest PDFs from Google Drive link.

GET /healthz â†’ health check.

POST /setup_elser â†’ sets up ELSER endpoint + ingest pipeline.

UI (Streamlit)

Ask questions interactively.

Toggle retrieval mode (hybrid / ELSER).

Adjust Top-K.

Show grounded answer with citations.

Debug mode â†’ inspect retrieved chunks.
